{
    "libraries": {
        "pandas": [
            {
                "task": "Load a CSV file into a DataFrame",
                "code": "df = pd.read_csv('filename.csv')",
                "complexity": "low",
                "keywords": [
                    "load",
                    "read",
                    "csv"
                ]
            },
            {
                "task": "Clean column names (lowercase and snake_case)",
                "code": "df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace(r'[^a-z0-9_]', '', regex=True)",
                "complexity": "low",
                "keywords": [
                    "clean",
                    "column",
                    "names"
                ]
            },
            {
                "task": "Advanced imputation of missing values",
                "code": "for col in df.select_dtypes(include='number'):\n    df[col] = df[col].fillna(df[col].median())\nfor col in df.select_dtypes(include='object'):\n    df[col] = df[col].fillna(df[col].mode()[0])",
                "complexity": "medium",
                "keywords": [
                    "impute",
                    "missing",
                    "fillna",
                    "median"
                ]
            },
            {
                "task": "Pivot table with multiple aggregations",
                "code": "pivot = df.pivot_table(index=['{cat_col_1}', '{cat_col_2}'], values=['{num_col_1}', '{num_col_2}'], aggfunc={'num_col_1': 'mean', 'num_col_2': ['sum', 'count']})\nprint(pivot)",
                "complexity": "high",
                "keywords": [
                    "pivot",
                    "table",
                    "aggregate",
                    "multilevel"
                ]
            },
            {
                "task": "Filter with complex multiple conditions",
                "code": "filtered_df = df[(df['{num_col}'] > df['{num_col}'].quantile(0.75)) & (df['{cat_col}'].isin(['{val1}', '{val2}'])) & (~df['{col}'].isna())]",
                "complexity": "medium",
                "keywords": [
                    "filter",
                    "query",
                    "conditions",
                    "complex"
                ]
            },
            {
                "task": "Calculate rolling window statistics",
                "code": "df['rolling_mean'] = df.sort_values('{date_col}').groupby('{group_col}')['{val_col}'].rolling(window=7).mean().reset_index(0, drop=True)",
                "complexity": "high",
                "keywords": [
                    "rolling",
                    "window",
                    "moving average",
                    "time series"
                ]
            }
        ],
        "numpy": [
            {
                "task": "Calculate weighted average",
                "code": "weighted_avg = np.average(df['{val_col}'], weights=df['{weight_col}'])\nprint(f'Weighted Average: {weighted_avg}')",
                "complexity": "medium",
                "keywords": [
                    "weighted",
                    "average",
                    "numpy"
                ]
            },
            {
                "task": "Detect outliers using Z-score",
                "code": "z_scores = np.abs((df['{col}'] - df['{col}'].mean()) / df['{col}'].std())\noutliers = df[z_scores > 3]\nprint(f'Found {len(outliers)} outliers')",
                "complexity": "medium",
                "keywords": [
                    "outliers",
                    "zscore",
                    "anomaly"
                ]
            }
        ],
        "scikit-learn": [
            {
                "task": "Train a robust Random Forest Pipeline with Preprocessing",
                "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\nX = df.drop(columns=['{target_col}'])\ny = df['{target_col}']\n\nnum_feats = X.select_dtypes(include=['int64', 'float64']).columns\ncat_feats = X.select_dtypes(include=['object', 'category']).columns\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), num_feats),\n        ('cat', Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('onehot', OneHotEncoder(handle_unknown='ignore'))]), cat_feats)\n    ])\n\nmodel = Pipeline([('preprocessor', preprocessor), ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\nprint(f'R2 Score: {r2_score(y_test, preds):.4f}')\nprint(f'MAE: {mean_absolute_error(y_test, preds):.4f}')",
                "complexity": "v_high",
                "keywords": [
                    "pipeline",
                    "random forest",
                    "preprocessing",
                    "onehot",
                    "impute"
                ]
            },
            {
                "task": "Perform Grid Search for Hyperparameter Tuning",
                "code": "from sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import classification_report\n\nX = df.drop(columns=['{target_col}'])\ny = df['{target_col}']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nparam_grid = {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]}\ngrid = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=3, scoring='accuracy')\ngrid.fit(X_train, y_train)\n\nprint(f'Best Params: {grid.best_params_}')\nprint(classification_report(y_test, grid.predict(X_test)))",
                "complexity": "v_high",
                "keywords": [
                    "grid search",
                    "tuning",
                    "optimization",
                    "hyperparameters"
                ]
            },
            {
                "task": "K-Means Clustering with Elbow Method Visualization",
                "code": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\nX = df[['{col1}', '{col2}', '{col3}']].dropna()\ninertias = []\nK = range(1, 11)\nfor k in K:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X)\n    inertias.append(kmeans.inertia_)\n\nplt.figure(figsize=(10,6))\nplt.plot(K, inertias, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Inertia')\nplt.title('Elbow Method for Optimal k')\nplt.show()",
                "complexity": "high",
                "keywords": [
                    "kmeans",
                    "clustering",
                    "elbow",
                    "unsupervised"
                ]
            }
        ],
        "statsmodels": [
            {
                "task": "Comprehensive OLS Regression with Assumption Checks",
                "code": "import statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX = df[['{x_col1}', '{x_col2}', '{x_col3}']]\ny = df['{y_col}']\nX = sm.add_constant(X)\n\n# Fit Model\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n# VIF Check\nvif_data = pd.DataFrame()\nvif_data['feature'] = X.columns\nvif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\nprint('\\nVariance Inflation Factors:\\n', vif_data)",
                "complexity": "high",
                "keywords": [
                    "ols",
                    "regression",
                    "vif",
                    "assumptions",
                    "statsmodels"
                ]
            },
            {
                "task": "Logistic Regression with Odds Ratios",
                "code": "import statsmodels.api as sm\nX = df[['{x_col1}', '{x_col2}']]\ny = df['{binary_target}']\nX = sm.add_constant(X)\nlogit_model = sm.Logit(y, X).fit()\nprint(logit_model.summary())\nprint('\\nOdds Ratios:')\nprint(np.exp(logit_model.params))",
                "complexity": "high",
                "keywords": [
                    "logistic",
                    "logit",
                    "odds ratio",
                    "statsmodels"
                ]
            }
        ],
        "scipy": [
            {
                "task": "One-way ANOVA Test",
                "code": "from scipy import stats\ngroups = [group['{num_col}'].values for name, group in df.groupby('{cat_col}')]\nf_val, p_val = stats.f_oneway(*groups)\nprint(f'ANOVA results: F={f_val:.4f}, p={p_val:.4f}')",
                "complexity": "medium",
                "keywords": [
                    "anova",
                    "hypothesis",
                    "groups",
                    "variance"
                ]
            },
            {
                "task": "Chi-Square Test of Independence",
                "code": "from scipy.stats import chi2_contingency\ncont_table = pd.crosstab(df['{cat_col_1}'], df['{cat_col_2}'])\nchi2, p, dof, expected = chi2_contingency(cont_table)\nprint(f'Chi2: {chi2:.4f}, p-value: {p:.4f}')",
                "complexity": "medium",
                "keywords": [
                    "chi-square",
                    "independence",
                    "categorical"
                ]
            }
        ],
        "plotly": [
            {
                "task": "Interactive Dashboard with Subplots",
                "code": "import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=('Scatter Plot', 'Box Plot'))\n\nfig.add_trace(go.Scatter(x=df['{x_col}'], y=df['{y_col}'], mode='markers', name='Points'), row=1, col=1)\nfig.add_trace(go.Box(y=df['{y_col}'], name='Distribution'), row=1, col=2)\n\nfig.update_layout(title_text='Interactive Analysis Dashboard', height=600)\nfig.show()",
                "complexity": "high",
                "keywords": [
                    "dashboard",
                    "plotly",
                    "subplots",
                    "interactive"
                ]
            },
            {
                "task": "3D Scatter Plot with Color Dimension",
                "code": "import plotly.express as px\nfig = px.scatter_3d(df, x='{x_col}', y='{y_col}', z='{z_col}', color='{cat_col}', title='3D Analysis')\nfig.show()",
                "complexity": "high",
                "keywords": [
                    "3d",
                    "scatter",
                    "plotly",
                    "interactive"
                ]
            }
        ],
        "seaborn": [
            {
                "task": "Advanced Regression Plot with Marginals",
                "code": "g = sns.jointplot(data=df, x='{x_col}', y='{y_col}', kind='reg', truncate=False, color='m', height=7)\ng.set_axis_labels('{x_col}', '{y_col}')\nplt.show()",
                "complexity": "medium",
                "keywords": [
                    "jointplot",
                    "regression",
                    "marginal",
                    "distribution"
                ]
            },
            {
                "task": "Violin Plot with Split by Category",
                "code": "plt.figure(figsize=(10,6))\nsns.violinplot(data=df, x='{cat_col_1}', y='{num_col}', hue='{cat_col_2}', split=True, inner='quart')\nplt.title('Violin Plot Split by {cat_col_2}')\nplt.show()",
                "complexity": "medium",
                "keywords": [
                    "violin",
                    "distribution",
                    "split",
                    "category"
                ]
            }
        ],
        "date_time": [
            {
                "task": "Resample Time Series Data",
                "code": "df['{date_col}'] = pd.to_datetime(df['{date_col}'])\nmonthly_avg = df.set_index('{date_col}').resample('M')['{val_col}'].mean()\nprint(monthly_avg.head())",
                "complexity": "medium",
                "keywords": [
                    "resample",
                    "monthly",
                    "semporal",
                    "time series"
                ]
            }
        ]
    }
}